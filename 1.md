# 第一次打卡 by zinccat

### 计时函数

```python
import time

#定义Timer类
# define a timer class to record time
class Timer(object):
    """Record multiple running times."""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        # start the timer
        self.start_time = time.time()

    def stop(self):
        # stop the timer and record time into a list
        self.times.append(time.time() - self.start_time)
        return self.times[-1]

    def avg(self):
        # calculate the average and return
        return sum(self.times)/len(self.times)

    def sum(self):
        # return the sum of recorded time
        return sum(self.times)

timer = Timer()
#Write code here
‘%.5f sec’ % timer.stop()
#或使用time.time()
```

### import模板

```python
# import packages and modules
%matplotlib inline
import torch
from torch import nn
from IPython import display
from matplotlib import pyplot as plt
import numpy as np
import random
torch.manual_seed(1)

print(torch.__version__)
torch.set_default_tensor_type('torch.FloatTensor')
```

torch.randn(size,size,dtype=torch.float32)生成随机数据阵(0-1,正态分布)

plt.scatter(x,y)生成散点图

可以使用random.shuffle(list) 打乱数据

TensorName.requires_grad_(requires_grad=True) 标记以便后续梯度计算

np.random.normal(a, b, (x, y)) 生成a,b间随机数，形状为x,y 可用于初始化网络权值

torch.mm(a,b) 矩阵乘法

对数据操作(eg.优化(sgd))时使用TensorName.data替换TensorName可以停止这一步的gradient check

每个epoch跑完后要用TensorName.grad.data.zero_()对gradient清零，从而避免其累加

损失函数使用torch.nn.MSELoss()

注意对向量操作时一定要保证维度存在，即不为(n,)，避免运算时采用broadcasting而非正常向量运算

### 数据集的读取与训练超参数的设置

```python
# 读取数据
batch_size = 256
num_workers = 4
train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)
```

### 简单网络的模板

```python
# -*- coding: utf-8 -*-
import torch
from torch import nn
from torch.nn import init  # 用于网络参数的初始化
import torch.utils.data as Data  # 用于数据集的读取
import torch.optim as optim  # 优化函数
from matplotlib import pyplot as plt  # 绘图
import numpy as np

torch.manual_seed(1)

print(torch.__version__)
torch.set_default_tensor_type('torch.FloatTensor')

num_inputs = 3
num_examples = 100

true_w = [4.3, 2.1, 5.7]
true_b = 6.7

features = torch.randn(num_examples, num_inputs, dtype=torch.float32)

labels = true_w[0] * features[:, 0] + true_w[1] * \
    features[:, 1] + true_w[2] * features[:, 2] + true_b
labels += torch.tensor(np.random.normal(0, 0.02,size=labels.size()), dtype=torch.float32)

batch_size = 10

dataset = Data.TensorDataset(features, labels)

data_iter = Data.DataLoader(
    dataset=dataset,
    batch_size=batch_size,
    shuffle=True,
)

net = nn.Sequential(
    nn.Linear(num_inputs, 10),
    #nn.Relu(),
    #nn.Linear(10, 1)
)

print(net)

init.normal_(net[0].weight, mean=0.0, std=0.01)  # guassian initiation method
# init.constant_(net[0].bias, val = 0.0) #constant initiation method
#init.normal_(net[1].weight, mean = 0.0, std = 0.01)

loss = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

#记录损失，可用于绘制损失函数
losses = []

num_epochs = 50
for epoch in range(1, num_epochs+1):
    for X, y in data_iter:
        output = net(X)
        l = loss(output, y.view(-1, 1))
        optimizer.zero_grad()
        l.backward()
        optimizer.step()
    # losses.append(l.item())
    print('epoch %d, loss: %f' % (epoch, l.item()))

dense = net[0]
print(true_w, dense.weight.data)
print(true_b, dense.bias.data)
```

交叉熵损失函数读入的tensor类型为long(即0/1分类指标)